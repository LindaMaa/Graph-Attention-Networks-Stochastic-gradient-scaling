{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_3cnzeKhSUt"
      },
      "outputs": [],
      "source": [
        "# imports \n",
        "import torch\n",
        "def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "!pip install python-igraph leidenalg cairocffi\n",
        "!pip install dgl\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.utils as U\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import igraph as ig\n",
        "\n",
        "import dgl.nn as dglnn\n",
        "from dgl import AddSelfLoop\n",
        "from dgl.data import CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset\n",
        "import pdb\n",
        "from dgl.nn.pytorch import GATConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AfzMTD2Bid3b"
      },
      "outputs": [],
      "source": [
        "#constants\n",
        "DROPOUT_RATE=0.0\n",
        "N_EPOCHS=200\n",
        "REPEATS=100\n",
        "HIDDEN_UNITS=8\n",
        "HEADS=[8,1]\n",
        "\n",
        "# hyperparameters\n",
        "class lambda_value():\n",
        "  def __init__(self,lambda_p):\n",
        "    super().__init__()\n",
        "    self.lambda_p=lambda_p\n",
        "\n",
        "  def update_lambda(self,l):\n",
        "    self.lambda_p=l\n",
        "  \n",
        "  def get_lambda(self):\n",
        "    return self.lambda_p\n",
        "\n",
        "latest_lambda = lambda_value(1.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - BASE GAT MODEL \n",
        "class GATV2(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1:  \n",
        "                h = h.mean(1)\n",
        "            else: \n",
        "                h = h.flatten(1)\n",
        "        return h"
      ],
      "metadata": {
        "id": "LojKv-Qah8cH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fixp27gohXlh"
      },
      "outputs": [],
      "source": [
        "# 2 - FORWARD FEATURE SCALING\n",
        "class GATReg(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, lambda_parameter=1.01, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.lambda_parameter=lambda_parameter\n",
        "\n",
        "  \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        " \n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "           \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([self.lambda_parameter], device=\"cpu\"), torch.tensor([self.lambda_parameter], device=\"cpu\")) \n",
        "        scalar = beta.sample(h.shape[:1]) + 0.1\n",
        "        scalar[scalar > 1] = 1.0/(2.0-scalar[scalar > 1])\n",
        "\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1:  \n",
        "                h = h.mean(1)\n",
        "\n",
        "            else:  \n",
        "                h = h.flatten(1)\n",
        "        h = scalar * h\n",
        "        \n",
        "        return h "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3,4 - BACKWARD GRADIENTS SCALING + COMBINED SCALING\n",
        "class backward_reg(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(xx, x, training):\n",
        "        if training:\n",
        "            lambda_p = latest_lambda.get_lambda()\n",
        "            beta = torch.distributions.beta.Beta(torch.tensor([lambda_p], device=\"cpu\"), torch.tensor([lambda_p], device=\"cpu\"))\n",
        "            lambdaa = beta.sample(x.shape[:1]) + 0.1\n",
        "            lambdaa[lambdaa > 1] = 1.0/(2.0-lambdaa[lambdaa> 1])\n",
        "            x = lambdaa * x\n",
        "        \n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(xx, gradients):\n",
        "\n",
        "        lambda_p = latest_lambda.get_lambda()\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([lambda_p], device=\"cpu\"), torch.tensor([lambda_p], device=\"cpu\"))\n",
        "        lambdaa = beta.sample(gradients.shape[:1]) + 0.1\n",
        "        lambdaa[lambdaa > 1] = 1.0/(2.0-lambdaa[lambdaa > 1])\n",
        "\n",
        "        return lambdaa * gradients, 0\n",
        "\n",
        "class GATReg2(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, lambda_parameter=0.05, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.lambda_parameter=lambda_parameter\n",
        "       \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "           \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([self.lambda_parameter], device=\"cpu\"), torch.tensor([self.lambda_parameter], device=\"cpu\")) \n",
        "        scalar = beta.sample(h.shape[:1]) + 0.1\n",
        "        scalar[scalar > 1.0] = 1.0/(2.0-scalar[scalar > 1])\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1: \n",
        "                h = h.mean(1)\n",
        "            else: \n",
        "                h = h.flatten(1)\n",
        "        #h=h*scalar\n",
        "        return h"
      ],
      "metadata": {
        "id": "6qafybp_idDN"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}