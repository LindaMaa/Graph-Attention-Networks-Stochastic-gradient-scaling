{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2-d7WxMh58j"
      },
      "outputs": [],
      "source": [
        "# imports \n",
        "import torch\n",
        "def format_pytorch_version(version):\n",
        "    return version.split('+')[0]\n",
        "def format_cuda_version(version):\n",
        "    return 'cu' + version.replace('.', '')\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "!pip install dgl\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch_geometric.utils as U\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Dropout\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "import dgl.nn as dglnn\n",
        "import dgl\n",
        "from dgl import AddSelfLoop\n",
        "from dgl.data import CiteseerGraphDataset, CoraGraphDataset, PubmedGraphDataset\n",
        "import pdb\n",
        "from dgl.nn.pytorch import GATConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9wLRHbDXocvO"
      },
      "outputs": [],
      "source": [
        "#constants\n",
        "DROPOUT_RATE=0.0\n",
        "N_EPOCHS=50\n",
        "REPEATS=100\n",
        "HIDDEN_UNITS=8\n",
        "HEADS=[8,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fmcaxD-ujSay"
      },
      "outputs": [],
      "source": [
        "class lambda_value():\n",
        "  def __init__(self,lambda_p):\n",
        "    super().__init__()\n",
        "    self.lambda_p=lambda_p\n",
        "\n",
        "  def update_lambda(self,l):\n",
        "    self.lambda_p=l\n",
        "  \n",
        "  def get_lambda(self):\n",
        "    return self.lambda_p\n",
        "\n",
        "latest_lambda = lambda_value(1.01)\n",
        "  \n",
        "  \n",
        "class GATV2(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #middle layers\n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1:  \n",
        "                h = h.mean(1)\n",
        "            else: \n",
        "                h = h.flatten(1)\n",
        "        return h\n",
        "\n",
        "\n",
        "\n",
        "class GATReg(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, lambda_parameter=1.01, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.lambda_parameter=lambda_parameter\n",
        "\n",
        "  \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "        #middle layers\n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "           \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([self.lambda_parameter], device=\"cpu\"), torch.tensor([self.lambda_parameter], device=\"cpu\")) \n",
        "        scalar = beta.sample(h.shape[:1]) + 0.1\n",
        "        scalar[scalar > 1] = 1.0/(2.0-scalar[scalar > 1])\n",
        "\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1:  \n",
        "                h = h.mean(1)\n",
        "\n",
        "            else:  \n",
        "                h = h.flatten(1)\n",
        "        h = scalar * h\n",
        "        \n",
        "        return h\n",
        "\n",
        "\n",
        "class backward_reg(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(xx, x, training):\n",
        "        if training:\n",
        "            lambda_p = latest_lambda.get_lambda()\n",
        "            beta = torch.distributions.beta.Beta(torch.tensor([lambda_p], device=\"cpu\"), torch.tensor([lambda_p], device=\"cpu\"))\n",
        "            lambdaa = beta.sample(x.shape[:1]) + 0.5\n",
        "            lambdaa[lambdaa > 1] = 1./(2-lambdaa[lambdaa> 1])\n",
        "            x = lambdaa * x\n",
        "        \n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(xx, gradients):\n",
        "\n",
        "        lambda_p = latest_lambda.get_lambda()\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([lambda_p], device=\"cpu\"), torch.tensor([lambda_p], device=\"cpu\"))\n",
        "        lambdaa = beta.sample(gradients.shape[:1]) + .5\n",
        "        lambdaa[lambdaa > 1] = 1./(2-lambdaa[lambdaa > 1])\n",
        "\n",
        "        return lambdaa * gradients, None\n",
        "\n",
        "\n",
        "class GATReg2(nn.Module):\n",
        "    def __init__(self, in_size, hid_size, out_size, heads, lambda_parameter=0.05, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList()\n",
        "        self.lambda_parameter=lambda_parameter\n",
        "        \n",
        "        # two-layer GAT\n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                in_size,\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        #middle layers\n",
        "        for n in range(num_layers-2):\n",
        "           self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                hid_size,\n",
        "                heads[0],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=F.elu,\n",
        "            )\n",
        "        )\n",
        "           \n",
        "        self.gat_layers.append(\n",
        "            dglnn.GATv2Conv(\n",
        "                hid_size * heads[0],\n",
        "                out_size,\n",
        "                heads[1],\n",
        "                feat_drop=DROPOUT_RATE,\n",
        "                attn_drop=DROPOUT_RATE,\n",
        "                activation=None,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        h = inputs\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([self.lambda_parameter], device=\"cpu\"), torch.tensor([self.lambda_parameter], device=\"cpu\")) \n",
        "        scalar = beta.sample(h.shape[:1]) + 0.5\n",
        "        scalar[scalar > 1.0] = 1.0/(2.0-scalar[scalar > 1])\n",
        "        for i, layer in enumerate(self.gat_layers):\n",
        "            h = layer(g, h)\n",
        "            if i == len(self.gat_layers)-1: \n",
        "                h = h.mean(1)\n",
        "            else: \n",
        "                h = h.flatten(1)\n",
        "        #h=h*scalar\n",
        "        return h\n",
        "\n",
        "def evaluate(g, features, labels, mask, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(g, features)\n",
        "        logits = logits[mask]\n",
        "        labels = labels[mask]\n",
        "        _, indices = torch.max(logits, dim=1)\n",
        "        correct = torch.sum(indices == labels)\n",
        "        return correct.item() * 1.0 / len(labels)\n",
        "\n",
        "\n",
        "def train(g, features, labels, masks, model):\n",
        "    # define train/val samples, loss function and optimizer\n",
        "    train_mask = masks[0]\n",
        "    val_mask = masks[1]\n",
        "    loss_fcn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        model.train()\n",
        "        bw_reg=backward_reg(model.lambda_parameter)\n",
        "        logits = model(g, features)\n",
        "        beta = torch.distributions.beta.Beta(torch.tensor([model.lambda_parameter], device=\"cpu\"), torch.tensor([model.lambda_parameter], device=\"cpu\")) \n",
        "        scalar = beta.sample(logits.shape[:1]) + .1\n",
        "        scalar[scalar > 1] = 1./(2.0-scalar[scalar > 1])\n",
        "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        bw_reg.apply(logits[train_mask], model.training)\n",
        "        optimizer.step()\n",
        "        acc = evaluate(g, features, labels, val_mask, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSct5tzQW4-g",
        "outputId": "9bc45e01-8aaa-4607-d240-dff388363479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dgl in /usr/local/lib/python3.8/dist-packages (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (1.7.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (5.9.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from dgl) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.8/dist-packages (from dgl) (2.8.8)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from dgl) (4.64.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->dgl) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install dgl\n",
        "import dgl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRPPirMojVTL"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "accuracies={}\n",
        "noise=[i/100 for i in range(0,80)]\n",
        "for noise_level in noise:\n",
        "    nl_acc_avg=0\n",
        "    nl_accs=[]\n",
        "  # transform dataset\n",
        "    transform = (dgl.AddEdge(noise_level))    \n",
        "    data=PubmedGraphDataset(transform=transform)\n",
        "    g = data[0]\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    g = g.int().to(device)\n",
        "    features = g.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n",
        "    in_size = features.shape[1]\n",
        "    out_size = data.num_classes\n",
        "    \n",
        "    for i in range(REPEATS):\n",
        "      model = GATReg2(in_size, HIDDEN_UNITS, out_size, heads=[8, 1], num_layers=2).to(device)\n",
        "\n",
        "    # model training\n",
        "      train(g, features, labels, masks, model)\n",
        "      acc = evaluate(g, features, labels, masks[2], model)\n",
        "      nl_accs.append(acc)\n",
        "      \n",
        "    print(\"Noise level: \", noise_level)\n",
        "    mean_acc=np.mean(np.asarray(nl_accs))\n",
        "    accuracies[noise_level]=mean_acc\n",
        "    std_acc=np.std(np.asarray(nl_accs))\n",
        "    print(f'\\nMean test accuracy: {mean_acc}%\\n')\n",
        "    print(f'\\nStd test accuracy: {std_acc}%\\n')\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6mVjbCx9-1L",
        "outputId": "75c5a6ca-70de-4a0d-9a2c-59cbb9e95623"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0: 0.7626,\n",
              " 0.01: 0.7552000000000001,\n",
              " 0.02: 0.7534,\n",
              " 0.03: 0.7426,\n",
              " 0.04: 0.7343999999999999,\n",
              " 0.05: 0.7236,\n",
              " 0.06: 0.7162,\n",
              " 0.07: 0.709,\n",
              " 0.08: 0.7144,\n",
              " 0.09: 0.7124,\n",
              " 0.1: 0.725,\n",
              " 0.11: 0.7203999999999999,\n",
              " 0.12: 0.6996,\n",
              " 0.13: 0.6996,\n",
              " 0.14: 0.7037999999999999,\n",
              " 0.15: 0.7013999999999999,\n",
              " 0.16: 0.6752,\n",
              " 0.17: 0.6779999999999999,\n",
              " 0.18: 0.6811999999999999,\n",
              " 0.19: 0.6846,\n",
              " 0.2: 0.6828000000000001,\n",
              " 0.21: 0.6378,\n",
              " 0.22: 0.677,\n",
              " 0.23: 0.6746,\n",
              " 0.24: 0.6512,\n",
              " 0.25: 0.6764,\n",
              " 0.26: 0.6808,\n",
              " 0.27: 0.6312,\n",
              " 0.28: 0.6486000000000001,\n",
              " 0.29: 0.6456,\n",
              " 0.3: 0.6416,\n",
              " 0.31: 0.6424000000000001,\n",
              " 0.32: 0.6186,\n",
              " 0.33: 0.6366,\n",
              " 0.34: 0.657,\n",
              " 0.35: 0.6182000000000001,\n",
              " 0.36: 0.6266,\n",
              " 0.37: 0.6148,\n",
              " 0.38: 0.589,\n",
              " 0.39: 0.6368,\n",
              " 0.4: 0.6089999999999999,\n",
              " 0.41: 0.6104,\n",
              " 0.42: 0.5935999999999999,\n",
              " 0.43: 0.6164,\n",
              " 0.44: 0.6279999999999999,\n",
              " 0.45: 0.6164,\n",
              " 0.46: 0.6104,\n",
              " 0.47: 0.6154,\n",
              " 0.48: 0.6262,\n",
              " 0.49: 0.6126,\n",
              " 0.5: 0.6138,\n",
              " 0.51: 0.6100000000000001,\n",
              " 0.52: 0.6024,\n",
              " 0.53: 0.6175999999999999,\n",
              " 0.54: 0.6076,\n",
              " 0.55: 0.6202,\n",
              " 0.56: 0.6226,\n",
              " 0.57: 0.5963999999999999,\n",
              " 0.58: 0.6125999999999999,\n",
              " 0.59: 0.6086,\n",
              " 0.6: 0.591,\n",
              " 0.61: 0.5978,\n",
              " 0.62: 0.5972000000000001,\n",
              " 0.63: 0.6186,\n",
              " 0.64: 0.6157999999999999,\n",
              " 0.65: 0.6136,\n",
              " 0.66: 0.5745999999999999,\n",
              " 0.67: 0.5899999999999999,\n",
              " 0.68: 0.6144000000000001,\n",
              " 0.69: 0.5978000000000001,\n",
              " 0.7: 0.5756,\n",
              " 0.71: 0.5902000000000001,\n",
              " 0.72: 0.567,\n",
              " 0.73: 0.5772,\n",
              " 0.74: 0.5801999999999999,\n",
              " 0.75: 0.5933999999999999,\n",
              " 0.76: 0.586,\n",
              " 0.77: 0.596,\n",
              " 0.78: 0.6054,\n",
              " 0.79: 0.5900000000000001}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1iPcRChXUCK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# lambda 0.005\n",
        "accuracies={}\n",
        "noise=[0.0,0.01,0.02,0.05,0.08,0.1,0.12,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.6]\n",
        "for noise_level in noise:\n",
        "    nl_acc_avg=0\n",
        "    nl_accs=[]\n",
        "  # transform dataset\n",
        "    transform = (dgl.AddEdge(noise_level))    \n",
        "    data = CoraGraphDataset(transform=transform)\n",
        "    g = data[0]\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    g = g.int().to(device)\n",
        "    features = g.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "    masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    g = g.int().to(device)\n",
        "    features = g.ndata[\"feat\"]\n",
        "    labels = g.ndata[\"label\"]\n",
        "\n",
        "    masks = g.ndata[\"train_mask\"], g.ndata[\"val_mask\"], g.ndata[\"test_mask\"]\n",
        "\n",
        "    in_size = features.shape[1]\n",
        "    out_size = data.num_classes\n",
        "    for i in range(REPEATS):\n",
        "      model = GATReg(in_size, HIDDEN_UNITS, out_size, heads=[8, 1], num_layers=2).to(device)\n",
        "\n",
        "    # model training\n",
        "      train(g, features, labels, masks, model)\n",
        "      acc = evaluate(g, features, labels, masks[2], model)\n",
        "      nl_accs.append(acc)\n",
        "      \n",
        "    print(\"Noise level: \", noise_level)\n",
        "    mean_acc=np.mean(np.asarray(nl_accs))\n",
        "    accuracies[noise_level]=mean_acc\n",
        "    std_acc=np.std(np.asarray(nl_accs))\n",
        "    print(f'\\nMean test accuracy: {mean_acc}%\\n')\n",
        "    print(f'\\nStd test accuracy: {std_acc}%\\n')\n",
        "        "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}